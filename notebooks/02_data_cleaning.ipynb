{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "658d7323",
   "metadata": {},
   "source": [
    "# Data Cleaning Notebook\n",
    "\n",
    "This notebook provides a systematic approach to cleaning your data based on findings from the exploration phase.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#setup-and-data-loading)\n",
    "2. [Missing Values Treatment](#missing-values-treatment)\n",
    "3. [Outlier Detection and Treatment](#outlier-detection-and-treatment)\n",
    "4. [Duplicate Detection](#duplicate-detection)\n",
    "5. [Data Type Corrections](#data-type-corrections)\n",
    "6. [Data Validation](#data-validation)\n",
    "7. [Export Cleaned Data](#export-cleaned-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# EDA toolkit imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../plots')\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from data_cleaner import DataCleaner\n",
    "from statistical_analysis import StatisticalAnalyzer\n",
    "from box_plot import create_outlier_analysis_plot\n",
    "from utils import PlotConfig\n",
    "\n",
    "# Configure plotting\n",
    "plot_config = PlotConfig()\n",
    "plot_config.set_style()\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ed05f",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data (replace with your actual data path)\n",
    "loader = DataLoader()\n",
    "\n",
    "# For demonstration, create sample data with issues\n",
    "np.random.seed(42)\n",
    "sample_data = {\n",
    "    'ID': range(1, 1001),\n",
    "    'Age': np.random.randint(18, 80, 1000),\n",
    "    'Income': np.random.normal(50000, 15000, 1000),\n",
    "    'Education Level': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD', 'bachelor', 'HIGH SCHOOL'], 1000),\n",
    "    'Experience_Years': np.random.randint(0, 40, 1000),\n",
    "    'Salary': np.random.normal(60000, 20000, 1000),\n",
    "    'Category': np.random.choice(['A', 'B', 'C', 'a'], 1000)\n",
    "}\n",
    "\n",
    "# Add outliers and missing values\n",
    "sample_data['Income'][np.random.choice(1000, 50, replace=False)] = np.nan\n",
    "sample_data['Salary'][np.random.choice(1000, 30, replace=False)] = np.nan\n",
    "sample_data['Income'][np.random.choice(1000, 10, replace=False)] = np.random.uniform(200000, 500000, 10)\n",
    "sample_data['Age'][np.random.choice(1000, 5, replace=False)] = np.random.choice([150, 200], 5)\n",
    "\n",
    "# Add duplicates\n",
    "data = pd.DataFrame(sample_data)\n",
    "duplicate_rows = data.sample(20).copy()\n",
    "data = pd.concat([data, duplicate_rows], ignore_index=True)\n",
    "\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "print(f\"Data loaded with intentional issues for cleaning demonstration\")\n",
    "\n",
    "# Initialize cleaner\n",
    "cleaner = DataCleaner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cebf89",
   "metadata": {},
   "source": [
    "## Missing Values Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing value patterns\n",
    "missing_patterns = cleaner.detect_missing_patterns(data)\n",
    "\n",
    "print(\"Missing Values Analysis:\")\n",
    "display(missing_patterns['by_column'])\n",
    "\n",
    "print(f\"\\nRows with any missing values: {missing_patterns['rows_with_missing']} ({missing_patterns['rows_with_missing_percentage']:.2f}%)\")\n",
    "print(f\"Columns with no missing values: {missing_patterns['complete_columns']}\")\n",
    "print(f\"Highly missing columns (>50%): {missing_patterns['highly_missing_columns']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c358b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define missing value treatment strategy\n",
    "missing_strategy = {\n",
    "    'Income': 'median',  # Use median for income\n",
    "    'Salary': 'median',  # Use median for salary\n",
    "    'Education Level': 'mode',  # Use mode for categorical\n",
    "    'Category': 'mode'\n",
    "}\n",
    "\n",
    "# Handle missing values\n",
    "data_cleaned = cleaner.handle_missing_values(data, strategy=missing_strategy, threshold=0.5)\n",
    "\n",
    "print(f\"After missing value treatment:\")\n",
    "print(f\"Shape: {data_cleaned.shape}\")\n",
    "print(f\"Missing values remaining: {data_cleaned.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ec810",
   "metadata": {},
   "source": [
    "## Outlier Detection and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b7f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in numeric columns\n",
    "numeric_cols = data_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "outlier_analysis = cleaner.detect_outliers(data_cleaned, columns=numeric_cols, method='iqr')\n",
    "\n",
    "print(\"Outlier Analysis (IQR method):\")\n",
    "for col, info in outlier_analysis.items():\n",
    "    if info['count'] > 0:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Outliers found: {info['count']} ({info['percentage']:.2f}%)\")\n",
    "        print(f\"  Outlier values: {info['values'][:5]}{'...' if len(info['values']) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6515a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers for key variables\n",
    "for col in ['Age', 'Income', 'Salary']:\n",
    "    if col in data_cleaned.columns:\n",
    "        print(f\"\\nOutlier Analysis for {col}:\")\n",
    "        create_outlier_analysis_plot(data_cleaned, col, \n",
    "                                   save_path=f'../figures/temp/{col}_outlier_analysis.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers (choose appropriate strategy)\n",
    "# Options: 'remove', 'cap', 'transform'\n",
    "outlier_strategy = 'cap'  # Cap outliers to reasonable bounds\n",
    "\n",
    "data_cleaned = cleaner.handle_outliers(data_cleaned, outlier_analysis, strategy=outlier_strategy)\n",
    "\n",
    "print(f\"After outlier treatment ({outlier_strategy}):\")\n",
    "print(f\"Shape: {data_cleaned.shape}\")\n",
    "\n",
    "# Re-check outliers\n",
    "new_outlier_analysis = cleaner.detect_outliers(data_cleaned, columns=numeric_cols, method='iqr')\n",
    "for col, info in new_outlier_analysis.items():\n",
    "    print(f\"{col}: {info['count']} outliers remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0893aa52",
   "metadata": {},
   "source": [
    "## Duplicate Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect duplicates\n",
    "duplicate_info = cleaner.detect_duplicates(data_cleaned)\n",
    "\n",
    "print(f\"Duplicate Analysis:\")\n",
    "print(f\"Duplicate rows found: {duplicate_info['count']} ({duplicate_info['percentage']:.2f}%)\")\n",
    "\n",
    "if duplicate_info['count'] > 0:\n",
    "    print(f\"\\nFirst few duplicate rows:\")\n",
    "    display(data_cleaned[data_cleaned.duplicated()].head())\n",
    "    \n",
    "    # Remove duplicates\n",
    "    data_cleaned = cleaner.remove_duplicates(data_cleaned, keep='first')\n",
    "    print(f\"\\nAfter removing duplicates:\")\n",
    "    print(f\"Shape: {data_cleaned.shape}\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0353c",
   "metadata": {},
   "source": [
    "## Data Type Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c4537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names\n",
    "data_cleaned = cleaner.standardize_column_names(data_cleaned)\n",
    "\n",
    "print(\"Column names standardized:\")\n",
    "print(data_cleaned.columns.tolist())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(data_cleaned.dtypes)\n",
    "\n",
    "# Fix categorical data inconsistencies\n",
    "if 'education_level' in data_cleaned.columns:\n",
    "    print(\"\\nEducation level values before cleaning:\")\n",
    "    print(data_cleaned['education_level'].value_counts())\n",
    "    \n",
    "    # Standardize education levels\n",
    "    education_mapping = {\n",
    "        'high school': 'High School',\n",
    "        'HIGH SCHOOL': 'High School',\n",
    "        'bachelor': 'Bachelor',\n",
    "        'Bachelor': 'Bachelor',\n",
    "        'Master': 'Master',\n",
    "        'PhD': 'PhD'\n",
    "    }\n",
    "    \n",
    "    data_cleaned['education_level'] = data_cleaned['education_level'].map(education_mapping).fillna(data_cleaned['education_level'])\n",
    "    \n",
    "    print(\"\\nEducation level values after cleaning:\")\n",
    "    print(data_cleaned['education_level'].value_counts())\n",
    "\n",
    "# Standardize category values\n",
    "if 'category' in data_cleaned.columns:\n",
    "    print(\"\\nCategory values before cleaning:\")\n",
    "    print(data_cleaned['category'].value_counts())\n",
    "    \n",
    "    data_cleaned['category'] = data_cleaned['category'].str.upper()\n",
    "    \n",
    "    print(\"\\nCategory values after cleaning:\")\n",
    "    print(data_cleaned['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdbdb8",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate cleaned data\n",
    "print(\"Data Validation Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check for remaining issues\n",
    "print(f\"1. Shape: {data_cleaned.shape}\")\n",
    "print(f\"2. Missing values: {data_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"3. Duplicates: {data_cleaned.duplicated().sum()}\")\n",
    "\n",
    "# Validate numeric ranges\n",
    "numeric_cols = data_cleaned.select_dtypes(include=[np.number]).columns\n",
    "print(f\"\\n4. Numeric column ranges:\")\n",
    "for col in numeric_cols:\n",
    "    min_val = data_cleaned[col].min()\n",
    "    max_val = data_cleaned[col].max()\n",
    "    print(f\"   {col}: {min_val:.2f} to {max_val:.2f}\")\n",
    "\n",
    "# Validate categorical consistency\n",
    "categorical_cols = data_cleaned.select_dtypes(include=['object', 'category']).columns\n",
    "print(f\"\\n5. Categorical column unique values:\")\n",
    "for col in categorical_cols:\n",
    "    unique_count = data_cleaned[col].nunique()\n",
    "    print(f\"   {col}: {unique_count} unique values\")\n",
    "\n",
    "# Generate cleaning report\n",
    "cleaning_report = cleaner.get_cleaning_report()\n",
    "print(f\"\\n6. Cleaning Summary:\")\n",
    "for step, details in cleaning_report.items():\n",
    "    print(f\"   {step}: {details}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a750cb",
   "metadata": {},
   "source": [
    "## Export Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned data\n",
    "output_path = '../data/processed/cleaned_data.csv'\n",
    "data_cleaned.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data exported to: {output_path}\")\n",
    "print(f\"Final shape: {data_cleaned.shape}\")\n",
    "\n",
    "# Create before/after comparison\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Rows', 'Columns', 'Missing Values', 'Duplicates'],\n",
    "    'Before': [data.shape[0], data.shape[1], data.isnull().sum().sum(), data.duplicated().sum()],\n",
    "    'After': [data_cleaned.shape[0], data_cleaned.shape[1], \n",
    "             data_cleaned.isnull().sum().sum(), data_cleaned.duplicated().sum()]\n",
    "})\n",
    "\n",
    "print(\"\\nBefore vs After Comparison:\")\n",
    "display(comparison)\n",
    "\n",
    "print(\"\\nData cleaning completed successfully!\")\n",
    "print(\"Ready for feature analysis and modeling.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
